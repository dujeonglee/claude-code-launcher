# Claude Code Launcher

A GUI application for configuring and launching Claude Code with on-premise LLM providers.

## Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Run the launcher
python claude_launcher.py
```

In the GUI:
1. Select your LLM runtime (Ollama, vLLM, etc.)
2. Enter your server's Base URL
3. Click "Validate" to test the connection
4. Select models for Haiku, Sonnet, and Opus
5. Click "Let's Roll" to launch Claude Code

## Features

- **Multi-runtime Support**: Works with Ollama, vLLM, MLX Serve, llama.cpp, LM Studio, and Text Generation Inference
- **Automatic Installation**: Downloads and installs Claude Code CLI automatically
- **Version Management**: Checks for Claude Code updates from the official CHANGELOG
- **Model Mapping**: Assign specific models to Claude's Haiku, Sonnet, and Opus roles
- **Config Persistence**: Saves configuration to `~/.claude/launcher_config.yaml`
- **Platform Detection**: Automatic OS and architecture detection for downloads

## Installation

### Prerequisites

- Python 3.9+
- Qt6 (for PySide6)
- pip package manager

### Dependencies

```bash
pip install PySide6 requests PyYAML
```

### Supported Platforms

- macOS (Intel and Apple Silicon)
- Linux (x86_64 and ARM64)
- Windows (x86_64)

## Usage

### Basic Usage

1. Launch the application:
   ```bash
   python claude_launcher.py
   ```

2. Configure your server:
   - **Runtime**: Select your LLM provider
   - **Base URL**: Enter the server URL (e.g., `http://localhost:11434` for Ollama)
   - Click "Validate" to verify connectivity

3. Set model mappings:
   - Click "Refresh Models from Server" to fetch available models
   - Select which model should be used for each Claude role (Haiku, Sonnet, Opus)

4. Configure Claude CLI:
   - Path to Claude CLI (auto-detected if in PATH)
   - Permission mode (default, plan, delegate, trust, bypassPermissions)
   - Max turns (0 for unlimited)

5. Launch:
   - Click "Let's Roll" to generate the launch script and open Claude Code

### Advanced Usage

#### Manual Claude Installation

If automatic installation fails or your platform is unsupported:

```bash
npm install -g @anthropic-ai/claude-code
```

Then set the path in the launcher GUI.

#### Custom API Keys

For non-Ollama runtimes that require authentication, set the API key in the GUI. This will be exported as `ANTHROPIC_API_KEY` in the generated script.

## Generated Script Format

The launcher generates a bash script at `~/.claude/launch_claude.sh`:

```bash
#!/bin/bash
# Generated by Claude Code Launcher v2.0.0
# Runtime: Ollama

# --- Environment ---
export ANTHROPIC_BASE_URL="http://localhost:11434"
export ANTHROPIC_AUTH_TOKEN="ollama"
export ANTHROPIC_DEFAULT_HAIKU_MODEL="claude-3-haiku-20240307"
export ANTHROPIC_DEFAULT_SONNET_MODEL="claude-3-sonnet-20240229"

cd "/Users/username/work"

# --- Launch Claude Code ---
claude
```

## Configuration

Configuration is stored in `~/.claude/launcher_config.yaml`:

```yaml
server:
  runtime: ollama
  base_url: http://localhost:11434
  api_key: ""  # Optional, for non-Ollama runtimes

models:
  haiku: ""
  sonnet: ""
  opus: ""

claude_code:
  path: claude
  permission_mode: default
  max_turns: 0

working_directory: /Users/username
```

### Configuration Options

| Field | Description |
|-------|-------------|
| `server.runtime` | LLM provider: ollama, vllm, mlx, llama_cpp, lm_studio, tgi |
| `server.base_url` | Base URL of your LLM server |
| `server.api_key` | API key (required for non-Ollama runtimes) |
| `models.haiku/sonnetopus` | Model names for each Claude role |
| `claude_code.path` | Path to Claude CLI binary |
| `claude_code.permission_mode` | Permission handling: default, plan, delegate, trust, bypassPermissions |
| `claude_code.max_turns` | Maximum conversation turns (0 = unlimited) |
| `working_directory` | Default directory for Claude sessions |

## Architecture

### Core Components

```
┌─────────────────────────────────────────────────────────────┐
│                    ClaudeLauncherWindow                     │
│                      (PySide6 GUI)                          │
└────────────────────┬────────────────────────────────────────┘
                     │
         ┌───────────┼───────────┐
         │           │           │
         ▼           ▼           ▼
┌─────────────┐ ┌─────────┐ ┌──────────────┐
│ConfigManager│ │Version  │ │ScriptGenerator│
│             │ │Manager  │ │               │
│ Load/Save   │ │Check &  │ │Generate bash │
│ YAML config │ │Update   │ │launch script │
└─────────────┘ │Fetching │ └──────────────┘
                └────┬────┘
                     │
                     ▼
              ┌──────────────┐
              │ ClaudeInstaller
              │ (QThread)    │
              │ Download &   │
              │ Extract      │
              └──────────────┘
```

### Supported LLM Runtimes

| Runtime | Default Port | API Path | Health Check |
|---------|-------------|----------|--------------|
| Ollama | 11434 | /api/tags | /api/tags |
| vLLM | 8000 | /v1/models | /health |
| MLX Serve | 8080 | /v1/models | /v1/models |
| llama.cpp | 8000 | /v1/models | /health |
| LM Studio | 1234 | /v1/models | /v1/models |
| TGI | 8000 | /v1/models | /health |

## Development

### Running Tests

```bash
# Syntax check
python -m py_compile claude_launcher.py

# Type checking (if mypy configured)
mypy claude_launcher.py
```

### Code Structure

- `OSChecker` - OS and architecture detection
- `VersionManager` - Claude Code version checking from CHANGELOG
- `ConfigManager` - YAML configuration loading and saving
- `ClaudeInstaller` - Background download/install thread
- `ServerProbe` - Server health and model listing
- `ScriptGenerator` - Bash script generation
- `ClaudeLauncherWindow` - Main GUI application

## Troubleshooting

### Server Connection Fails

- Verify your LLM server is running
- Check the Base URL is correct (include port)
- For Ollama: `ollama serve`
- For vLLM: `python -m vllm.entrypoints.api_server`

### Claude Not Found

```bash
# Install via npm
npm install -g @anthropic-ai/claude-code

# Or check PATH
which claude
```

### Download Fails

- Check internet connectivity
- Verify your platform is supported (macOS, Linux, Windows)
- The download URL is constructed as: `{base_url}/{version}/{platform}/claude`
  - Example: `https://storage.googleapis.com/claude-code-dist-86c565f3-f756-42ad-8dfa-d59b1c096819/claude-code-releases/v2.1.42/darwin-arm64/claude`

### Model List Empty

- Verify server is accessible
- Check the server has models loaded
- For Ollama: `ollama pull <model-name>`

## License

This project is provided as-is for personal and internal use.

### Third-Party Licenses

This project uses PySide6 (Qt for Python) which is licensed under LGPL v3. When
distributing this application, you must comply with LGPL v3 terms, particularly
regarding dynamic linking and user ability to replace the Qt library.

## Contributing

Contributions are welcome! Here's how to get started:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes
4. Run tests (`python -m py_compile claude_launcher.py`)
5. Commit your changes (`git commit -m 'Add some amazing feature'`)
6. Push to the branch (`git push origin feature/amazing-feature`)
7. Open a Pull Request

### Development Setup

```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Run linter
pylint claude_launcher.py

# Run type checker
mypy claude_launcher.py
```

### Coding Standards

- Follow PEP 8 style guide
- Use type hints on all functions
- Add docstrings for public methods
- Test changes before submitting

### Security Considerations

- Never commit API keys or secrets
- Run `python -m pylint claude_launcher.py` before committing
- Check for sensitive data with `grep -r "api_key\|password" .`
